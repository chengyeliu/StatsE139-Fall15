\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}


\pdfinfo{
  /Title (example.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (David Wihl)
  /Subject (Cheat Sheet)
  /Keywords (pdflatex, latex,pdftex,tex)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}


% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols*}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}
\setlength{\columnseprule}{1pt}


\subsubsection{Basics}

\begin{tabular}{| c | c | c| }
\hline
 & Sample statistic &  Population Parameter \\
 \hline
 Mean & $\overbar{X}$ & $\mu$ \\
 Variance & $S^2$ & $\sigma^2$ \\
 Correlation & $r$ & $\rho$ \\
  Noise & $e_i$ & $\epsilon_i$ \\
\hline
& Guess & True, but unknown \\
\hline
\end{tabular}

The Mean, Variance and StdDev are subject to outliers.
\begin{align*}
\overbar{X} =& \frac{1}{n} \sum\limits_{i=1}^n x_i \\
\mu = E(W) =& E(a + bX) = a + bE(X)\\
\sigma^2 = Var(X) =& E[X^2] - \mu_x^2\\
S^2 =& \sum\limits_{all\ i} \frac{(X_i - \overbar{X})^2}{n-1}\\
\sigma = StdDev(X) =& \sqrt{\sigma^2}\\
Var(X) =& E[(X - \mu)^2]  = E(X^2) - \mu^2\\
	=& \sum_{all x} (x - \mu_x)^2 P(X = x)\\
	=& E(X^2) - E(X)^2 = E(X - E(X))^2\\
Var(X + c) =& Var(X)\\
Var(cX) =& c^2Var(X)\\
Var(\bar{X}) =& \frac{\sigma^2}{n} \\
 \text{If X,Y are dependent}\\
Var(X+Y) \ne& Var(X) + Var(Y)\\
Var(X + Y) =& Var(X - Y)\\
Var((a + bX) + (c + dY) =& b^2Var(X) + d^2Var(Y) + 2bdCov(X,Y)\\
\end{align*}
Quartiles split the data into 4 equal groups by number of values, or 25\% percentiles.
Q2 is the median.


\subsubsection{Covariance and Correlation}
Covariance gives direction.\\
Correlation gives direction and strength.\\
Both are \textit{linear}.
\begin{align*}
Cov(X, Y) =& \frac{1}{N} \sum\limits_{i=1}^{N} {(x_i - \overbar{x})(y_i - \overbar{y})} \\
Cor =& \frac{Cov}{\sigma_x \cdot \sigma_y}\\
\end{align*}


\subsubsection{Probabilities}
$ 0 \leq $ All probabilities $ \leq 1 $\\
\textit{Mutually exclusive and Exhaustive}\\
\begin{align*}
P(\overbar{A}) =& 1 - P(A)\\
P(A \text{ or } B) =& P(A) + P(B) - P(A \text{ and } B)\\
P(A \vert B) =& \frac{P(A \text{ and } B)}{P(B)}\\
\end{align*}
Joint vs. Marginal probabilities\\
Independent if $P(A\vert B) = P(A)$\\
For Independent only: $P(E \text{ and } F) = P(E)\cdot P(F)$\\
\begin{tabular}{| c | c | c |}

\hline
&B &$\overbar{B}$\\
\hline
A & $P(A and B) = P(B)P(A \vert B)$ & $P(A and \overbar{B}) = P(\overbar{B})P(A \vert \overbar{B})$\\
$\overbar{A}$ & $P(\overbar{A} and B) = P(\overbar{A})P(B\vert \overbar{A})$ & $ P(\overbar{A}  and \overbar{B}) = P(\overbar{A})P(\overbar{B} \vert \overbar{A})$ \\
\hline
\end{tabular}

\begin{tabular}{| c | c |}
\hline
all success & $p^n$\\
all failure & $(1-p)^n$\\
at least one failure & $1-p^n$\\
at least one success & $1 - (1-p)^n$\\
\hline
\end{tabular}


\subsubsection{Random Variables}
\begin{align*}
P(X \le x) \rightarrow& CDF\\
E(cX) =& c\cdot E(X)\\
E(X+c) =& E(X) + c\\
E(X+Y) =& E(X) + E(Y)\\
\end{align*}



\subsubsection{Bias of an Estimator}
In practice $n$ has to relatively much larger like $> 100$.



Guesses should be \textit{unbiased} and have \textit{minimum variance}. MVUE (Minimum Variance, Unbiased Estimates).
\[
bias(\hat{\theta}) = E(\hat{\theta}) - \theta
\]
Unbiased if $bias = 0$ (expected value equals true, not a particular value of $\overbar{x}$). 

For samples, we divide by $n-1$ instead of $n$ to make an unbiased estimator. The guess would otherwise be too low.
\[
E(X) = \sum_{all x} xP(X=x) 
\]
\[
E(\overbar{X}) = \sum\limits_{i=1}^{\infty} x_i p_i =  \mu
\]
\[
E(S^2) = \sigma^2
\]
\begin{align*}
E[X + c] =& E[X] + c\\
E[X + Y] =& E[X] + E[Y]\\
E[aX] =& a E[X]\\
E[aX + bY + c] =& aE[X] + bE[Y] +c\\
E((a + bX) + (c_dY)) =& a + bE(X) + c + dE(Y)\\
\end{align*}
Example: Roulette has a \$1 bet with a \$35 payoff for $\frac{1}{38}$ odds.
\[
E[\text{gain from a \$1 bet}] = -\$1 \cdot \frac{37}{38} + \$35 \cdot \frac{1}{38} = -\$0.0526
\]

\subsubsection{All Things $\chi^2$}

\begin{align*}
\chi^2_1 =& Z^2 \\
\chi^2_n =& \sum\limits_{i=1}^n Z^2 \\
t_n =& \frac{Z} {\sqrt{\chi^2_n / n} } \\
F_{n,m} =& \frac{ \chi^2_n / n} { \chi^2_m / m } \\
\chi^2_{n+m} =& \chi^2_n + \chi^2_m \\
\frac{(n-1) S^2} {\sigma^2} \sim& X^2_{df = n-1} \\
S^2 = \frac{1}{n-1}\sum\limits_{i=1}^n (X_i - \overbar{x})^2 \sim& \frac{\sigma^2}{n-1}\chi^2_{df=n-1} \\
\end{align*}

\subsection{The $t$ distribution}
The $t$ distribution has a slightly wider spread in the tails than Standard Normal. Also known as student-t distribution. This happens when you don't know the
real $\sigma$ and are forced to use $S$, which is less reliable.

\[
\overbar{X} \approx N(\mu, \frac{\sigma^2}{n})
\]

allows us to Z-score
\[
Z = \frac{\overbar{X} - \mu}{\sigma / \sqrt{n}}
\]
but we don't know $\sigma$ so in practice we use
\[
T = \frac{\overbar{X} - \mu}{S / \sqrt{n}}
\]
(which has two random variables ($\overbar{X}$ and $S$). Now, take that previous definition of $T$ and add $\sigma/\sqrt{n}$ to the top and bottom:

\begin{align*}
T =& \frac{\overbar{X} - \mu}{S / \sqrt{n}} \\
   =&  \frac{(\overbar{X} - \mu) / (\sigma/\sqrt{n})}{(S / \sqrt{n}) / (\sigma/\sqrt{n})}\\
   =& \frac{Z}{\sqrt{S^2/\sigma^2}}\\
   =& \frac{Z}{\sqrt{\frac{(n-1)S^2}{(n-1)\sigma^2}}} \\
 =&  \frac{Z}{\sqrt{\chi^2/df}}\text{, since }\chi^2 = \frac{(n-1)S^2}{\sigma^2}
\end{align*}

\subsection{Hypothesis Test - General}

\begin{enumerate}
\item Formulate hypotheses
\item Calculate test statistic
\item Calculate $p$-value based on reference distribution of test statistic $|H_0$. \textbf{$p$ value is NOT the probability that the null hypothesis is true.} 
It is probablity of seeing our data on the null hypothesis. 
\item Determine conclusion and scope of inference
\end{enumerate}
\textbf{Note:} $t$ values are measures of \textit{distance}. $p$ values are measure of \textit{probability}.

\begin{tabular}{ c  c }
Hypotheses & Decision Rule \\
\hline
$H_0 : \mu = \mu_0$ \\      $H_a : \mu \neq \mu_0$ & If $|t_{stat}| > 1.96$, reject $H_0$ \\
 & \\
$H_0 : \mu = \mu_0$ \\ $H_a : \mu < \mu_0$      & If $t_{stat} < -1.64$, reject $H_0$ \\
 & \\
$H_0 : \mu = \mu_0$ \\ $H_a : \mu > \mu_0$      & If $t_{stat} > 1.64$, reject $H_0$ \\
\end{tabular}

\textbf{Scope of Inference}: \textbf{Interval Validity}: low when 1) unaccounted confounding factors, 2) ignored missing data 3) noncompliance 4) unverified assumptions
5) suboptimal method of analysis. Applicable when allocation of units to groups is random.
 \textbf{External Validity}: high when can be generalized to population. Applicable when selection of units is random.

Assumptions:
\begin{enumerate}
\item Observations are \textbf{independent}. This is true if the sample is randomly selected, but false if there is bias in the sampling.
\item \textbf{Normal Distributions} of the observations. This happens when the sample is large enough (via the Central Limit Theorm) or it is known that the population is normally distributed.
\end{enumerate}

If these assumptions are not correct, none of the $t$-tests will work correctly.


\subsubsection{Hypothesis Test - Mean}

Calculation by hand using a $t$ test:
\begin{align*}
t_{stat} =& \frac{ \overbar{x} - \mu_0  } { s / \sqrt{n}  } \\
\end{align*}

\subsection{Types of Errors}
\begin{description}
\item[Type I]the null hypothesis is rejected when it is true
\item[Type II] the null hypothesis is accepted when it is false
\end{description}
$\alpha$ is \textit{level of significance} - probability of making a Type I error. The greater the cost of an error, the smaller $\alpha$ should be.
$\beta$ is the probability of making a Type II error. There is an inverse relation between Type I and II errors. Reducing one increases the other. The only way to reduce both is to increase $n$ the sample size.


\subsubsection{Confidence Intervals}

CI is Estimate $\pm$ margin of error or Estimate $\pm$ ($z$ value) $\times$ (SD of estimate).

\[
\mu_0 = \overbar{X} \pm t\cdot s/\sqrt{n}
\]

If the interval spans 0, then one is not significantly bigger than the other (or cannot be determined). 
As long as $n > 30$, it doesn't matter if the sample size is different between the random variables.

To find the 95\% Confidence Internval for the \textbf{true mean} $\mu$:
\[
\overbar{x} \pm t^* (s/\sqrt{n})
\]

$t^*$ has to be looked up

\subsubsection{Comparing Two Proportions}
\[
Var(\hat{p}_1 - \hat{p}_2) = \frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2} 
\]

The 95\% confidence interval for $p_1-p_2$ is:
\[
(\hat{p}_1 - \hat{p}_2) \pm 1.96 \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}   }
\]

Decision Rules for Testing Two Proportions:
\[
T = \frac{(\hat{p}_1 - \hat{p}_2) }  { \sqrt{ \hat{p}(1-\hat{p}) (\frac{1}{n_1} + \frac{1}{n_2}) }} \text{, where } \hat{p} = \frac{n_1\hat{p}_1 + n_2\hat{p_2} } {n_1 + n_2 }
\]
$\hat{p}$ is called the \textit{pooled proportion}.

\subsubsection{Comparing Two Means}
Requirements:
\begin{enumerate}
\item{$\sigma_1$ and $\sigma_2$ are unknown. No assumption made about their equality.}
\item{The two samples are independent.}
\item{Both samples are simple random samples.}
\item{The two samples size are both large (ie. $> 30$) or both populations have normal distributions.}
\end{enumerate}

A confidence interval for $(\mu_1 - \mu_2 )$ is
\[
(\overbar{x}_1 - \overbar{x}_2) \pm 1.96 \sqrt{ \frac{s^2_1 } {n_1 } + \frac{s^2_2 } {n_2 }     }
\]

\subsubsection{Comparing Two Normal Distributions}

\begin{align*}
A \sim& \mathcal{N}(\mu_A, \sigma_A^2) \\
B \sim& \mathcal{N}(\mu_B, \sigma_B^2) \\
\end{align*}
Find $P(A < B + c)$:
\begin{align*}
P&(A - B - c < 0)\\
(A - B - c) \sim& \mathcal{N}(\mu_A - \mu_B - c, \sigma_A^2 + \sigma_B^2 - 2Cov(A,B)\\
P&(X = A - B - c > 0) \\
\end{align*}
then Z-score

\subsubsection{Matched Pairs}

This when there are two samples that are \textbf{not} independent, e.g. Weight Watchers, Before / After or matched, shared characteristics.

Is the data matched or independent?

If we don't take into account the match, the results are wrong.

To account for this, take the difference between $\overbar{X}_1 - \overbar{X}_2$ and then do a hypothesis test on the \textit{difference}. 
\begin{align*}
H_0 : & \mu_D = 0 \\
H_a : & \mu_D > 0 \\
\end{align*}



\subsubsection{Uniform Distribution}
\begin{align*}
E(X) =& \frac{(a+b)}{2}\\
Var(X) =& \frac{(b-a)^2}{12} \\
\end{align*}
\ \ $y$ axis should be a fraction to make area $=1$

\subsubsection{Normal Distribution}
\begin{align*}
X \sim& \mathcal{N}(\mu,\sigma^2)\\
Z = &\frac{X - \mu}{\sigma} \sim \mathcal{N}(0,1)\\
X = &\sigma Z + \mu \\
P(a \leq X \leq b) =& P[(a-\mu) \leq (X-\mu) \leq (b-\mu)]\\
	= & P[\frac{(a-\mu)}{\sigma} \leq \frac{(X-\mu)} {\sigma} \leq \frac {(b-\mu)} {\sigma} ]\\
	= & P[\frac{(a-\mu)}{\sigma} \leq Z \leq \frac {(b-\mu)} {\sigma} ]\\
\end{align*}


\subsubsection{Binomial Distribution}
\begin{itemize}
\item{$n$ independent trials}
\item{binary result}
\item{same probability of success}
\item{total number of successes}
\end{itemize}
\begin{align*}
X \sim& Bin(n,p)\\
\binom{n}{x} =& \frac{n!}{x!(n-x)!}\\
P(X=x) =& \frac{n!}{x!(n-x)!}p^xq^{(n-x)}\\
\mu_x = E(X) =& n \cdot p\\
\sigma^2 = Var(X) =& n \cdot p \cdot q = n \cdot p \cdot (1-p)\\
\end{align*}
Shape of distribution depends on $p,n$.\\
Small $p$, left skewed. Large $p$, right skewed


\subsection{Fisher's Randomization Test}

Randomization Test is for experiments - for casual inference.

Permutation Tests are for observational studies - for generalization.

Fisher Randomization Test is a \textit{distribution free} test for treatment effect in \textbf{randomized experiments}. No assumption as to the underlying distribution has to be made. Generally, is $\mu_1 = \mu_2$? 

The only assumption is Additive Treatment Effect ($\delta$):
\[
Y_{c,i} = Y_{v,i} + \delta
\]

$H_0: \delta = 0$, ie  zero treatment effect for all units. Each unit's outcome is the same regardless of the treatment assigned. So the distribution would be identical in both groups.

$H_a: \delta \neq 0$ non-zero treatment effect for ALL units. 

Assumptions:
\begin{itemize}
\item Random assignment to groups
\item Under $H_0$, \textit{independence, interchangability} of study units. 
\end{itemize}

\subsubsection{Test Statistic}

Difference of outcomes of the two groups:
\[
\hat{\delta} = \overbar{Y}_c - \overbar{Y}_v
\]
could also use difference between the medians.

\textbf{Randomization Distribution} is the reference distribution of a test statistic in a randomization test,
where variation is due to random assignment of the treatment.

Procedure:
$Y$ is values of malaria. $X$ is if someone is vaccinated. Simulate a new $X^*$ with the same values as $X$ but \textit{in a different order}, leaving the same $Y$. Repeat multiple times to simulate and build the histogram. This will show if the distribution is extreme.

The maximum number of possible simulations is the binomial distribution:
\[{t \choose s}\]
where $t$ is the total number of study units and $s$ is the number where the effect was shown.

\subsubsection{Calculating $p$ value}
 
The $p$ value is proportion of values above or below the observed test statistic.
\begin{align*}
\hat{\delta} =& 0.5\\
p =& P(\overbar{Y}_c - \overbar{Y}_v \geq \overbar{y}_c - \overbar{y}_v) \\
   =& P(\overbar{Y}_c - \overbar{Y}_v \geq 0.5) = 0.029
\end{align*}

so there is sufficient evidence to reject the null.

Scope of inference: Internal Validity maybe not if the same hospital. External validity: possibly not, as these were volunteers.


% You can even have references
%\rule{0.3\linewidth}{0.25pt}
\scriptsize
\bibliographystyle{abstract}
\bibliography{refFile}
\end{multicols*}
\end{document}